package com.aallam.openai.api.answer

import com.aallam.openai.api.engine.EngineId
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

@Serializable
public data class AnswerRequest(

    /**
     * ID of the engine to use for completion.
     */
    @SerialName("model") public val model: EngineId,

    /**
     * Question to get answered.
     */
    @SerialName("question") public val question: String,

    /**
     * List of (question, answer) pairs that will help steer the model towards the tone and answer format you'd like.
     * We recommend adding 2 to 3 examples.
     */
    @SerialName("examples") public val examples: List<QuestionAnswer>,

    /**
     * A text snippet containing the contextual information used to generate the answers for the examples you provide.
     */
    @SerialName("examples_context") public val examplesContext: String,

    /**
     * List of documents from which the answer for the input [question] should be derived.
     * If this is an empty list, the question will be answered based on the question-answer examples.
     *
     * You should specify either [documents] or a [file], but not both.
     */
    @SerialName("documents") public val documents: List<String>? = null,

    /**
     * The ID of an uploaded file that contains documents to search over.
     * See upload file for how to upload a file of the desired format and purpose.
     *
     * You should specify either [documents] or a [file], but not both.
     */
    @SerialName("file") public val file: String? = null,

    /**
     * ID of the engine to use for semantic search.
     *
     * Defaults to ada.
     */
    @SerialName("search_model") public val searchModel: EngineId? = null,

    /**
     * The maximum number of documents to be ranked by semantic search when using [file].
     * Setting it to a higher value leads to improved accuracy but with increased latency and cost.
     *
     * Defaults to 200
     */
    @SerialName("max_rerank") public val maxRerank: Int? = null,

    /**
     * What sampling temperature to use. Higher values mean the model will take more risks and value 0 (argmax sampling)
     * works better for scenarios with a well-defined answer.
     *
     * Defaults to 0
     */
    @SerialName("temperature") public val temperature: Double? = null,

    /**
     * Include the log probabilities on the [logprobs] most likely tokens, as well the chosen tokens.
     * For example, if [logprobs] is 10, the API will return a list of the 10 most likely tokens.
     * The API will always return the logprob of the sampled token, so there may be up to [logprobs]+1 elements
     * in the response.
     *
     * When [logprobs] is set, `completion` will be automatically added into [expand] to get the logprobs.
     *
     * Defaults to `null`.
     */
    @SerialName("logprobs") public val logprobs: Int? = null,

    /**
     * The maximum number of tokens allowed for the generated answer.
     *
     * Defaults to 16.
     */
    @SerialName("max_tokens") public val maxTokens: Int? = null,

    /**
     * Up to 4 sequences where the API will stop generating further tokens.
     * The returned text will not contain the stop sequence.
     *
     * Defaults to `null`.
     */
    @SerialName("stop") public val stop: List<String>? = null,

    /**
     * How many answers to generate for each question.
     *
     * Defaults to 1.
     */
    @SerialName("n") public val n: Int? = null,

    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias`
     * value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text
     * to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling.
     * The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood
     * of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     *
     * As an example, you can pass `{"50256": -100}` to prevent the `<|endoftext|> token from being generated.
     *
     * Defaults to `null`.
     */
    @SerialName("logit_bias") public val logitBias: Map<String, Int>? = null,

    /**
     * A special boolean flag for showing metadata.
     * If set to true, each document entry in the returned JSON will contain a "metadata" field.
     *
     * This flag only takes effect when [file] is set.
     *
     * Defaults to `false`.
     */
    @SerialName("return_metadata") public val returnMetadata: Boolean? = null,

    /**
     * If set to `true`, the returned JSON will include a "prompt" field containing the final prompt that was used
     * to request a completion. This is mainly useful for debugging purposes.
     *
     * Defaults to `false`.
     */
    @SerialName("return_prompt") public val returnPrompt: Boolean? = null,

    /**
     * If an object name is in the list, we provide the full information of the object; otherwise, we only provide
     * the object ID. Currently we support `completion` and [file] objects for expansion.
     *
     * Defaults to empty list.
     */
    @SerialName("expand") public val expand: List<String>? = null
)
