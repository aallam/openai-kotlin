package com.aallam.openai.api.core

import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

@Serializable
public data class Usage(
    /**
     * Count of prompts tokens.
     */
    @SerialName("prompt_tokens") public val promptTokens: Int? = null,
    /**
     * Count of completion tokens.
     */
    @SerialName("completion_tokens") public val completionTokens: Int? = null,
    /**
     * Count of total tokens.
     */
    @SerialName("total_tokens") public val totalTokens: Int? = null,
    /**
     * Breakdown of tokens used in the prompt.
     */
    @SerialName("prompt_tokens_details") public val promptTokensDetails: PromptTokensDetails? = null,
    /**
     * Breakdown of tokens used in a completion.
     */
    @SerialName("completion_tokens_details") public val completionTokensDetails: CompletionTokensDetails? = null,
)

@Serializable
public data class PromptTokensDetails(
    /**
     * Cached tokens present in the prompt.
     */
    public val cachedTokens: Int? = null,
    /**
     * Audio input tokens present in the prompt.
     */
    public val audioTokens: Int? = null,
)

@Serializable
public data class CompletionTokensDetails(
    /**
     * Tokens generated by the model for reasoning.
     */
    public val reasoningTokens: Int? = null,
    /**
     * Audio input tokens generated by the model.
     */
    public val audioTokens: Int? = null,
    /**
     * When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion.
     */
    public val acceptedPredictionTokens: Int? = null,
    /**
     * When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion.
     * However, like reasoning tokens, these tokens are still counted in the total completion tokens for
     * purposes of billing, output, and context window limits.
     */
    public val rejectedPredictionTokens: Int? = null,
)
