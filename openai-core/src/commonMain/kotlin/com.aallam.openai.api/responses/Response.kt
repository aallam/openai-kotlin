package com.aallam.openai.api.responses

import com.aallam.openai.api.core.Status
import com.aallam.openai.api.model.ModelId
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

/**
 * Response from the OpenAI Responses API
 */
@Serializable
public data class Response(

    /**
     * Whether to run the model response in the background.
     */
    @SerialName("background")
    val background: Boolean?,

    /**
     * The Unix timestamp (in seconds) of when the response was created
     */
    @SerialName("created_at")
    val createdAt: Long,

    /**
     * An error object returned when the model fails to generate a Response.
     *
     */
    @SerialName("error")
    val error: ResponseError?,

    /**
     * Unique identifier for this Response.
     */
    @SerialName("id")
    val id: String,

    /**
     * Details about why the response is incomplete.
     *
     */
    @SerialName("incomplete_details")
    val incompleteDetails: IncompleteDetails?,

    /**
     * Inserts a system (or developer) message as the first item in the model's context.
     *
     * When using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses.
     */
    @SerialName("instructions")
    val instructions: String?,

    /**
     * An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
     */
    @SerialName("max_output_tokens")
    val maxOutputTokens: Long? = null,

    /**
     * The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
     */
    @SerialName("max_tool_calls")
    val maxToolCalls: Long? = null,

    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.
     *
     * Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
     */
    @SerialName("metadata")
    val metadata: Map<String, String> = emptyMap(),

    /**
     * Model ID used to generate the response, like gpt-4o or o1. OpenAI offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.
     */
    @SerialName("model")
    val model: ModelId,

    /**
     * The object type, always "response"
     */
    @SerialName("object")
    val objectType: String = "response",

    /**
     * An array of content items generated by the model.
     *
     * The length and order of items in the output array is dependent on the model's response.
     */
    @SerialName("output")
    val output: List<ResponseOutput> = emptyList(),

    /**
     * Whether parallel tool calls were enabled
     */
    @SerialName("parallel_tool_calls")
    val parallelToolCalls: Boolean,

    /**
     * The unique ID of the previous response to the model. Use this to create multi-turn conversations.
     */
    @SerialName("previous_response_id")
    val previousResponseId: String?,

    /**
     * Reference to a prompt template and its variables.
     */
    @SerialName("prompt")
    val prompt: PromptTemplate? = null,

    /** Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the `user` field. */
    @SerialName("prompt_cache_key")
    val promptCacheKey: String?,

    /**
     * Configuration options for reasoning models.
     */
    @SerialName("reasoning")
    val reasoning: ReasoningConfig?,

    /**
     * A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies.
     * The IDs should be a string that uniquely identifies each user.
     * We recommend hashing their username or email address, in order to avoid sending us any identifying information.
     */
    @SerialName("safety_identifier")
    val safetyIdentifier: String?,

    /**
     * Specifies the processing type used for serving the request.
     *
     * If set to 'auto', then the request will be processed with the service tier configured in the Project settings. Unless otherwise configured, the Project will use 'default'.
     * If set to 'default', then the request will be processed with the standard pricing and performance for the selected model.
     * If set to 'flex' or 'priority', then the request will be processed with the corresponding service tier. Contact sales to learn more about Priority processing.
     * When not set, the default behavior is 'auto'.
     * When the service_tier parameter is set, the response body will include the service_tier value based on the processing mode actually used to serve the request. This response value may be different from the value set in the parameter.
     */
    @SerialName("service_tier")
    val serviceTier: ServiceTier?,

    /**
     * The status of the response generation. One of `completed`, `failed`, `in_progress`, or `incomplete`.
     */
    @SerialName("status")
    val status: Status,

    /**
     * The temperature used for sampling
     */
    @SerialName("temperature")
    val temperature: Double,

    /**
     * Configuration options for a text response from the model. Can be plain text or structured JSON data.
     */
    @SerialName("text")
    val text: ResponseTextConfig? = null,

    /**
     * How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how to specify which tools the model can call.
     */
    @SerialName("tool_choice")
    val toolChoice: ResponseToolChoiceConfig,

    /**
     * An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter.
     *
     * The two categories of tools you can provide the model are:
     *
     * Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities, like web search or file search. Learn more about built-in tools.
     * Function calls (custom tools): Functions that are defined by you, enabling the model to call your own code. Learn more about function calling.
     */
    @SerialName("tools")
    val tools: List<ResponseTool>,

    /** An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. */
    @SerialName("top_logprobs")
    val topLogprobs: Int? = null,

    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     */
    @SerialName("top_p")
    val topP: Double,

    /**
     * The truncation strategy used for the model response.
     */
    @SerialName("truncation")
    val truncation: Truncation?,

    /**
     * Represents token usage details including input tokens, output tokens, a breakdown of output tokens, and the total tokens used.
     */
    @SerialName("usage")
    val usage: ResponseUsage?,

    /**
     * A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests and to help OpenAI detect and prevent abuse.
     */
    @Deprecated("This field is being replaced by safety_identifier and prompt_cache_key. Use prompt_cache_key instead to maintain caching optimizations.")
    @SerialName("user")
    val user: String?

)

/**
 * Details about why the response is incomplete
 */
@Serializable
public data class IncompleteDetails(
    /**
     * The reason why the response is incomplete
     */
    @SerialName("reason")
    val reason: String
)
