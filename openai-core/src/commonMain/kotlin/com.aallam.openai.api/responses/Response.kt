package com.aallam.openai.api.responses

import com.aallam.openai.api.core.Status
import com.aallam.openai.api.model.ModelId
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

/**
 * Response from the OpenAI Responses API
 */
@Serializable
public data class Response(

    /**
     * The Unix timestamp (in seconds) of when the response was created
     */
    @SerialName("created_at")
    val createdAt: Long,

    /**
     * An error object returned when the model fails to generate a Response.
     *
     */
    @SerialName("error")
    val error: ResponseError?,

    /**
     * Unique identifier for this Response.
     */
    @SerialName("id")
    val id: String,

    /**
     * Details about why the response is incomplete.
     *
     */
    @SerialName("incomplete_details")
    val incompleteDetails: IncompleteDetails?,

    /**
     * Inserts a system (or developer) message as the first item in the model's context.
     *
     * When using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses.
     */
    @SerialName("instructions")
    val instructions: String?,

    /**
     * An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
     */
    @SerialName("max_output_tokens")
    val maxOutputTokens: Long? = null,


    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.
     *
     * Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
     */
    @SerialName("metadata")
    val metadata: Map<String, String> = emptyMap(),

    /**
     * Model ID used to generate the response, like gpt-4o or o1. OpenAI offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.
     */
    @SerialName("model")
    val model: ModelId,

    /**
     * The object type, always "response"
     */
    @SerialName("object")
    val objectType: String = "response",


    /**
     * An array of content items generated by the model.
     *
     * The length and order of items in the output array is dependent on the model's response.
     */
    @SerialName("output")
    val output: List<ResponseOutput> = emptyList(),

    /**
     * Whether parallel tool calls were enabled
     */
    @SerialName("parallel_tool_calls")
    val parallelToolCalls: Boolean,

    /**
     * The unique ID of the previous response to the model. Use this to create multi-turn conversations.
     */
    @SerialName("previous_response_id")
    val previousResponseId: String?,

    /**
     * Configuration options for reasoning models.
     *
     */
    @SerialName("reasoning")
    val reasoning: ReasoningConfig?,

    /**
     * The status of the response generation. One of `completed`, `failed`, `in_progress`, or `incomplete`.
     */
    @SerialName("status")
    val status: Status,

    /**
     * The temperature used for sampling
     */
    @SerialName("temperature")
    val temperature: Double,
    /**
     * Configuration options for a text response from the model. Can be plain text or structured JSON data.
     */
    @SerialName("text")
    val text: ResponseTextConfig? = null,

    /**
     * How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how to specify which tools the model can call.
     */
    @SerialName("tool_choice")
    val toolChoice: ResponseToolChoiceConfig,

    /**
     * An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter.
     *
     * The two categories of tools you can provide the model are:
     *
     * Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities, like web search or file search. Learn more about built-in tools.
     * Function calls (custom tools): Functions that are defined by you, enabling the model to call your own code. Learn more about function calling.
     */
    @SerialName("tools")
    val tools: List<ResponseTool>,

    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     */
    @SerialName("top_p")
    val topP: Double,

    /**
     * The truncation strategy used for the model response.
     */
    @SerialName("truncation")
    val truncation: Truncation? = null,

    /**
     * Represents token usage details including input tokens, output tokens, a breakdown of output tokens, and the total tokens used.
     */
    @SerialName("usage")
    val usage: ResponseUsage? = null,

    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
     */
    @SerialName("user")
    val user: String? = null

)

/**
 * Details about why the response is incomplete
 */
@Serializable
public data class IncompleteDetails(
    /**
     * The reason why the response is incomplete
     */
    @SerialName("reason")
    val reason: String
)
